# -*- coding: utf-8 -*-
"""VACR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fx8KY0MMK3_8oO1T2awSVeUHT5sU8Ve2

Aircraft, and in particular airplanes, are alternative to objects typically considered for fine-grained categorization such as birds and pets. There are several aspects that make aircraft model recognition particularly interesting. 

* Firstly, aircraft designs span a hundred years, including many thousand different models and hundreds of different makes and airlines. 

* Secondly, aircraft designs vary significantly depending on the size (from home-built to large carriers), destination (private, civil, military), purpose (transporter, carrier, training, sport, fighter, etc.), propulsion (glider, propeller, jet), and many other factors including technology. One particular axis of variation, which is is not shared with categories such as animals, is the fact that the structure of the aircraft changes with their design (number of wings, undercarriages, wheel per undercarriage, engines, etc.). 

* Thirdly, any given aircraft model can be re-purposed or used by different companies, which causes further variations in appearance (livery). These, depending on the identification task, may be consider as noise or as useful information to be extracted. 

* Finally, aircraft are largely rigid objects, which simplifies certain aspects of their modeling (compared to highly-deformable animals such as cats), allowing one to focus on the core aspects of the fine-grained recognition problem.
"""



"""# Imports"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from matplotlib.pyplot import imread
from collections import Counter

import tensorflow as tf
import tensorflow_hub as hub

from IPython.display import Image
from PIL import Image as im
import glob
import os
import datetime


print("TF version:", tf.__version__)
print("TF version:", hub.__version__)

# Check for GPU availability
print("GPU", "available" if tf.config.list_physical_devices("GPU") else "not availiable")

"""## Load data from dataset"""

## ================================ Training Data ================================================================================
train_df = pd.read_csv('/content/drive/MyDrive/VACR/fgvc-aircraft-2013b 3/data/images_manufacturer_train.txt', names=['name'])
train_df[['image','manufacturer']] = train_df["name"].str.split(" ", 1, expand=True)
train_df.drop(columns=['name'], inplace=True)

## ================================ Validation Data ================================================================================
val_df = pd.read_csv('/content/drive/MyDrive/VACR/fgvc-aircraft-2013b 3/data/images_manufacturer_val.txt', names=['name'])
val_df[['image','manufacturer']] = val_df["name"].str.split(" ", 1, expand=True)
val_df.drop(columns=['name'], inplace=True)

## ================================ Testing Data ================================================================================
test_df = pd.read_csv('/content/drive/MyDrive/VACR/fgvc-aircraft-2013b 3/data/images_manufacturer_test.txt', names=['name'])
test_df[['image','manufacturer']] = test_df["name"].str.split(" ", 1, expand=True)
test_df.drop(columns=['name'], inplace=True)

## ================================= Bounding box ==================================================================================  
box_df = pd.read_csv('/content/drive/MyDrive/VACR/fgvc-aircraft-2013b 3/data/images_box.txt', names=['name'])
box_df[['image','xmin', 'ymin', 'xmax', 'ymax']] = box_df["name"].str.split(" ", 4, expand=True)
box_df.drop(columns=['name'], inplace=True)

## Train bounding box
train_box = pd.merge(train_df, box_df, how="left", on=["image"])
val_box = pd.merge(val_df, box_df, how="left", on=["image"])
test_box = pd.merge(test_df, box_df, how="left", on=["image"])

train_box.head()





train_df.shape, val_df.shape, test_df.shape, box_df.shape

"""## Explore Datasets"""

# How many labels do we have for each class
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,10))
fig.suptitle('Manufacturer Count', y=1, size=20)

pd.value_counts(train_df['manufacturer'], ascending=True).plot(kind='barh', ax=ax1)
pd.value_counts(val_df['manufacturer'], ascending=True).plot(kind='barh', ax=ax2)
pd.value_counts(test_df['manufacturer'], ascending=True).plot(kind='barh', ax=ax3);

ax1.set_title('Training data')
ax2.set_title('Validation data')
ax3.set_title('Testing data')
fig.tight_layout(h_pad=5.0, w_pad=5.0)

"""## Create Pathnames for the Image IDs in the frame"""

train_filenames = ["/content/drive/MyDrive/VACR/fgvc-aircraft-2013b 3/data/images/" + fname + ".jpg" for fname in train_df["image"]]

val_filenames = ["/content/drive/MyDrive/VACR/fgvc-aircraft-2013b 3/data/images/" + fname + ".jpg" for fname in val_df["image"]]

test_filenames = ["/content/drive/MyDrive/VACR/fgvc-aircraft-2013b 3/data/images/" + fname + ".jpg" for fname in test_df["image"]]

len(os.listdir('/content/drive/MyDrive/VACR/fgvc-aircraft-2013b 3/data/images/')), len(train_filenames), len(val_filenames), len(test_filenames)

print(train_filenames[:6])
train_df.head()

Image(train_filenames[1000])

train_filenames[1]

Image(train_filenames[1])

#Function to remove copyright label

def crop_image(image, box):

  img = im.open(image).convert('L')
  img = img.crop((int(box[0]), int(box[1]), int(box[2]), int(box[3])))
  img.save(image)

crop_image(train_filenames[0], train_box.iloc[0].to_numpy()[-4:])

Image(train_filenames[2])

# Crop all images to boundary boxes
#for pic, bbox in zip(train_filenames, train_box.values):
#  crop_image(pic, bbox[-4:])

Image(train_filenames[21])

Image(test_filenames[0])

# Crop all validation images to boundary boxes
#for pic, bbox in zip(val_filenames, val_box.values):
#  crop_image(pic, bbox[-4:])

# Crop all test images to boundary boxes
#for pic, bbox in zip(test_filenames, test_box.values):
#  crop_image(pic, bbox[-4:])

train_label  = np.array(train_df['manufacturer'])
val_label  = np.array(val_df['manufacturer'])
test_label  = np.array(test_df['manufacturer'])

"""Next we proceed to turn the labels into an array of booleans. This allows us to later turn the label into numerical data compatible with ML models"""

## ================================ Training Data ================================================================================
unique_labels_train = np.unique(train_label)
bool_labels_train = [each_label == unique_labels_train for each_label in train_label]


## ================================ Validation Data ================================================================================
unique_labels_val = np.unique(val_label)
bool_labels_val = [each_label == unique_labels_val for each_label in val_label]

## ================================ Test Data ================================================================================
unique_labels_test = np.unique(test_label)
bool_labels_test = [each_label == unique_labels_test for each_label in test_label]

bool_labels_train[0]



"""## Creation of Training, validation and Test sets 

The dataset now comprises of 3334 labelled images. In order to train, validate test a model, we would normally need to split the dataset into these 3 groups.

However, the datasets have already been split before being imported. `train_df` which so far comprises of only the training data will be used to train a model.
"""

X_train = train_filenames
y_train = bool_labels_train

X_val = val_filenames
y_val = bool_labels_val

X_test = test_filenames
y_test = bool_labels_test

X_val.shape

"""## Image Preprocessing into Tensors
To preprocess the images into Tensors, a function is created that that does the following:

1. Take image filepath as input
2. Use TensorFlow to read the file and save it into a variable, image
3. Convert image, originally in a jpg format, into Tensors
4. Normalize image (convert color chanel value from 0-225 to 0-1
5. Resize image to in the shape (224,224)
6. Return altered image

"""

# Define an imagesize based on input format for model we'll use
IMG_SIZE = 224

# Create a function for preprocessing images
def process_img(img_path):
    """
    Takes image filepath, converts it into a Tensor
    """

    # Read in image file
    img = tf.io.read_file(img_path)

    # Turn the jpeg image into numerical Tensor with 3 colour channels (Read, Green, Blue)
    img = tf.image.decode_jpeg(img, channels=3)           
  
    #Convert the color channel from 0-255 to 0-1 values (Normalization)
    img = tf.image.convert_image_dtype(img, tf.float32)
  
    # Resize image to defined value
    img = tf.image.resize(img, size=[IMG_SIZE, IMG_SIZE])

    return img

"""## Distribute data into batches
Here, the data is simply distributed into smaller batched to alleviate load on the memory
"""

# function that returns a tuple in the form(image,label)
def get_img_label(img_path, label):
    """
    takes an image file path name and the associated label, processes the image and returns a tuple
    of (image, label)
    """

    img = process_img(img_path)
    return img, label


# Define Batch size
BATCH_SIZE = 32

# Create function to turn data into batches
def create_data_batches(X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):
    """
    A batch is created out of image (X) and label (y) pairs.
    The data is shuffled if it is training data, but doesnt if it's validation data.
    Also accepts test data without labels as input
    """

    # Since data is a test dataset, most likely without labels
    if test_data:
        print("testing batches creating..") 
        data = tf.data.Dataset.from_tensor_slices((tf.constant(X),
                                                   tf.constant(y)))  # only filepaths (no labels)
        data_batch = data.map(get_img_label).batch(BATCH_SIZE)
        return data_batch

    # Validation data does not require shuffling
    elif valid_data:
        print("Validation batches creating..")
        data = tf.data.Dataset.from_tensor_slices((tf.constant(X),   # filepaths
                                                   tf.constant(y)))  # labels
        data_batch = data.map(get_img_label).batch(BATCH_SIZE)
        return data_batch

    else:
        print("training batches creating..")      
        # Convert filepaths and labels into Tensors
        data = tf.data.Dataset.from_tensor_slices((tf.constant(X),tf.constant(y)))
                                                  
        # before mapping image processor function, Shuffling pathnames and labels is faster than shuffling images
        data = data.shuffle(buffer_size=len(X))

        # Create (image, label) tuples (this also turns the image path into a preprocessed image)
        data = data.map(get_img_label)

        # Turn the training data into batches
        data_batch = data.batch(BATCH_SIZE)
    return data_batch

# Create training and validation data batches
train_data = create_data_batches(X_train, y_train)

val_data = create_data_batches(X_val, y_val, valid_data=True)

test_data = create_data_batches(X_test, y_test, test_data=True)



"""# Model Building
Prior to build the model, we specify:

* Input shape to the model (images shape in Tensors form)
* The output shape of the model (image labels in Tensors form)
* The model URL The URL of tensorflowHub: https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4
"""

# input shape to the model
INPUT_SHAPE = [None, IMG_SIZE,IMG_SIZE, 3]    # batch, height, width, colour channels

# output shape of our model
OUTPUT_SHAPE = len(unique_labels_train)

# model URL from TensorFlow Hub
MODEL_URL = "https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4"

# ===========================================================================================================
# Create a function for viewing images in a data batch
def show_25_images(images, labels):
    """
    Displays a plot of 25 images and their labels from a data batch
    """

    # Setup figure
    plt.figure(figsize=(10,10))

    # Loop through 25 (for displaying 25 images)
    for i in range(25):
        # Create subplots (5 rows, 5 columns)
        ax = plt.subplot(5,5,i+1)

        # Display an image
        plt.imshow(images[i])

        # Add the image label
        plt.title(unique_labels_train[labels[i].argmax()])

        plt.axis("off")


man_images, man_labels = next(train_data.as_numpy_iterator())

# Now to visualize data in a training batch
show_25_images(man_images, man_labels)

val_images, val_labels =  next(val_data.as_numpy_iterator())
show_25_images(val_images, val_labels)

"""With inputs, outputs and model ready, we combine them into a Keras deep learning model.

Let's create a function which:

1. Takes the input shape, output shape and model we've chosen as parameters
2. Defines the layers in a Keras model in sequential fashion (do this first, then this, then that).
3. Compiles the models(says it should be evaluated and improved)
4. Builds the model (tells the model the input shape it will be getting)
5. Return model
All the steps can be found here: https://www.tensorflow.org/guide/keras/sequential_model
"""

# Create a function which builds a Keras model
def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model=MODEL_URL):
    print("Building model with: " + MODEL_URL)

    # Setup the model layers
    model = tf.keras.Sequential([hub.KerasLayer(MODEL_URL),                      # Layer 1
                                 tf.keras.layers.Dense(units=OUTPUT_SHAPE,
                                                       activation="softmax")])   # Layer 2 (output layer)
                                        
    # Compile model
    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=["accuracy"])
        
    # Build model
    model.build(INPUT_SHAPE)

    return model

model = create_model()
model.summary()

#cnn_model.summary()

"""## Creating callbacks
Callbacks are helper functions a model can use during training to do such things as:

1. save its progress
2. check it's progress
3. or stop early if model stops improving
We'll create two callbacks, one for tensorboard which helps track our models progress and another for early early stopping which prevents model from training for too long

## TensorBoard Callback
To setup a tensorboard callback, we need to do three things:

1. Load the TensorBoard notebook extension 2.Create a TensorBoard callback able to save logs
2. Visualize models training logs with %tesorboard magic function (after model training)

"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Create a function to build a TensorBoard callback
def create_tensorboard_callback():
    #create a log directory for storing tensorboard logs
    logdir = os.path.join("/content/drive/MyDrive/VACR/logs",
                          # Make it so the logs get tracked whenever an experiment is run
                          datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    return tf.keras.callbacks.TensorBoard(log_dir=logdir)

# create earlystop callback
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",
                                                  patience=3)


# Create a function to build a TensorBoard callback
def create_tensorboard_callback2():
    #create a log directory for storing tensorboard logs
    logdir = os.path.join("/content/drive/MyDrive/VACR/logs",
                          # Make it so the logs get tracked whenever an experiment is run
                          datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    return tf.keras.callbacks.TensorBoard(log_dir=logdir)

# create earlystop callback
early_stopping2 = tf.keras.callbacks.EarlyStopping(monitor="accuracy",
                                                  patience=3)

"""## Model Training

## 1. CNN Benchmark
"""

from keras.models import Sequential
from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout

# build a sequential model
cnn_model = Sequential()
cnn_model.add(InputLayer(input_shape=(224, 224, 3)))

# 1st conv block
cnn_model.add(Conv2D(25, (5, 5), activation='relu', strides=(1, 1), padding='same'))
cnn_model.add(MaxPool2D(pool_size=(2, 2), padding='same'))
# 2nd conv block
cnn_model.add(Conv2D(50, (5, 5), activation='relu', strides=(2, 2), padding='same'))
cnn_model.add(MaxPool2D(pool_size=(2, 2), padding='same'))
cnn_model.add(BatchNormalization())
# 3rd conv block
cnn_model.add(Conv2D(70, (3, 3), activation='relu', strides=(2, 2), padding='same'))
cnn_model.add(MaxPool2D(pool_size=(2, 2), padding='valid'))
cnn_model.add(BatchNormalization())

# ANN block
cnn_model.add(Flatten())
cnn_model.add(Dense(units=100, activation='relu'))
cnn_model.add(Dense(units=100, activation='relu'))
cnn_model.add(Dropout(0.25))
# output layer
cnn_model.add(Dense(units=30, activation='softmax'))

# compile model
cnn_model.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy'])

# tensorboard callback
tensorboard = create_tensorboard_callback()

# fit on data for 30 epochs
cnn_model.fit(train_data, 
              epochs=90, 
              validation_data=val_data, 
              callbacks=[tensorboard, early_stopping])



"""## 2.  ImageNet Transfer Learning"""

NUM_EPOCHS = 90

# Build function to train and return a trained model
def train_model():
    """
    Trains a given model and returns the trained version
    """

    # Create a model
    model = create_model()

    # Create new TensorBoard function everytime we train a model
    tensorboard2 = create_tensorboard_callback()

    # Fit the model to the data passing it the callbacks we created
    model.fit(x=train_data,
              epochs=NUM_EPOCHS,
              validation_data=val_data,
              validation_freq=1,
              callbacks=[tensorboard2, early_stopping])
              
  
    #Return the fitted model
    return model

model = train_model()

"""## Predictions"""

cnn_predictions = cnn_model.predict(val_data, verbose=1)
trf_predictions = model.predict(val_data, verbose=1)

# ------------------------------------------------------------------------------------------------------------------------------------
def get_pred_label(prediction_probabilities):
    """
    Turns an array of prediction probabilities into a label
    """

    return unique_labels_train[np.argmax(prediction_probabilities)]

# Get a predicted label based on an array of prediction probabilities
cnn_pred_label = get_pred_label(cnn_predictions[81])
trf_pred_label = get_pred_label(trf_predictions[81])
#pred_label

# ------------------------------------------------------------------------------------------------------------------------------------
# Convert data backout of batches
def unbatchify(data):
    """
    Takes a batch dataset of (image, label) Tensors and returns seperate arrays of images and labels"
    """

    images_ = []
    labels_ = []

    # Loop through unbatched data
    for image, label in data.unbatch().as_numpy_iterator():
        images_.append(image)
        labels_.append(unique_labels_train[np.argmax(label)])
    return images_, labels_


# ------------------------------------------------------------------------------------------------------------------------------------
# Unbatchify validation data
val_images2, val_labels2 = unbatchify(val_data)
val_images2[0], val_labels2[0]

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/drive/MyDrive/VACR/logs





"""## Join training and validation datasets"""

full_train_data = train_data.concatenate(val_data)

"""### CNN Full Dataset"""

# tensorboard callback
tensorboard2 = create_tensorboard_callback2()

# fit on data for 30 epochs
cnn_model.fit(full_train_data, 
              epochs=90, 
              validation_data=test_data,
              callbacks=[tensorboard2, early_stopping])





cnn_model.evaluate(test_data)



"""### Transfer Learning Full dataset"""

# Build function to train and return a trained model
def train_model2():
    """
    Trains a given model and returns the trained version
    """

    # Create a model
    model = create_model()

    # Create new TensorBoard function everytime we train a model
    tensorboard2 = create_tensorboard_callback2()

    # Fit the model to the data passing it the callbacks we created
    model.fit(x=full_train_data,
              epochs=NUM_EPOCHS,
              validation_freq=1,
              validation_data=test_data,
              callbacks=[tensorboard2, early_stopping])
              
  
    #Return the fitted model
    return model

model2trf = train_model2()

model2trf.evaluate(test_data)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/drive/MyDrive/VACR/logs

def plot_pred(prediction_probabilities, labels,images, n=1):
    """
    View the prediction, ground truth, and image for sample n
    """
    pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]

     #Get the pred label
    pred_label = get_pred_label(pred_prob)

     # Plot image and remove ticks
    plt.imshow(image)
    plt.xticks([])
    plt.yticks([])

    # Change the color of the title depending on if the prediction is right or wrong
    if pred_label == true_label:
        color = "green"
    else:
        color = "red"

    # Change plot title to be predicted,probability of prediction and truth label
    plt.title("{} {:2.0f}%  Actual: {}".format(pred_label,
                                    np.max(pred_prob)*100,
                                    true_label), color=color)
                                         

def plot_pred_conf(prediction_probabilities, labels, n=1):
    """
    Plot the top 10 highest prediction confidences along withthe truth label for sample n.
    """
    pred_prob, true_label = prediction_probabilities[n], labels[n]

    pred_label = get_pred_label(pred_prob)               # Get the predicted label


    top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]       # Find the top 10 prediction indexes

  
    top_10_pred_values = pred_prob[top_10_pred_indexes]        # Find the top 10 prediction values

 
    top_10_pred_labels = unique_labels_train[top_10_pred_indexes]       # Find top 10 prediction labels

  
    top_plot = plt.bar(np.arange(len(top_10_pred_labels)),       # Set up plot
                        top_10_pred_values,
                        color="grey")
    plt.xticks(np.arange(len(top_10_pred_labels)),
                labels=top_10_pred_labels,
                rotation="vertical")
  
    #Change colour of true label
    if np.isin(true_label, top_10_pred_labels):
        top_plot[np.argmax(top_10_pred_labels == true_label)].set_color("green")
    else:
        pass

# Lets check out a few predictions and their different values
i_multiplier = 0
num_rows = 3
num_cols = 2
num_images = num_rows*num_cols
plt.figure(figsize=(10*num_cols, 5*num_rows))

for i in range(num_images):
    plt.subplot(num_rows, 2*num_cols, 2*i+1)
    plot_pred(prediction_probabilities=predictions,
              labels=val_labels2,
              images=val_images2,
              n=i+i_multiplier)  
    plt.subplot(num_rows, 2*num_cols, 2*i+2)
    plot_pred_conf(prediction_probabilities=predictions,
                   labels=val_labels2,
                   n=i+i_multiplier)

plt.tight_layout(h_pad=1.0)  
plt.show()

## Save Trained model
# Function to save a model
def save_model(model, suffix=None):
    """
    saves a given model in a models directory and appends a suffix (string).
    """

    # create a model directory pathname with current time
    modeldir = os.path.join("/content/drive/MyDrive/VACR/models", datetime.datetime.now().strftime("%Y%m%d-%H%M%s"))
    model_path = modeldir + "_" + suffix + ".h5" # save format of model
    print(f"saving model to: {model_path}")
    model.save(model_path)
    return model_path


# Function to load model
def load_model(model_path):
    """
    Loads a saved model from a specified path
    """
    print(f"Loading saved model from: {model_path}")
    model = tf.keras.models.load_model(model_path,
                                       custom_objects={"KerasLayer": hub.KerasLayer})
    return model

# Save model
save_model(model, suffix="manufacturer-train-images-mobilenetv2-Adam")

# Load Model
loaded_manufacturer_training_model = load_model('/content/drive/MyDrive/VACR/models/20210403-02521617418320_manufacturer-train-images-mobilenetv2-Adam.h5')

